## 哈希去重机制详解

### 核心设计目标

哈希去重机制的主要目的是：确保相同的代码段不会被重复索引，避免向量数据库中的冗余数据，提高搜索效率和存储利用率。

### 哈希生成策略

系统采用多维度组合哈希策略，根据不同的分段类型使用不同的哈希组合：

#### Tree-sitter解析分段哈希

**组合要素：**

- `filePath`：文件路径（确保跨文件不冲突）
- `start_line`：起始行号
- `end_line`：结束行号
- `content.length`：内容长度
- `contentPreview`：
  1. 内容前100字符预览（确保内容变化被检测）
  2. 回退分段哈希
  3. 超大行字符分特殊处理：增加了 `startCharIndex` 参数，确保同一行内不同字符段有不同的哈希值。

#### Markdown分段哈希

（具体策略待补充）

---

### 去重流程实现

**第一步：初始化哈希集合**  
在解析每个文件前，初始化一个空的哈希集合 `seenSegmentHashes`，用于记录已处理的代码段哈希。

**第二步：哈希存在性检查**  
对每一个生成的代码段，计算其哈希值，并检查是否已存在于 `seenSegmentHashes` 中。若存在，则跳过该段；否则继续处理。

**第三步：向量数据库中的唯一ID生成**  
为通过去重检查的代码段生成唯一标识符（如基于哈希值的UUID），并写入向量数据库。

---

### 智能设计特点

#### 内容敏感性
通过包含 `contentPreview`（前100字符）确保即使行号相同但内容不同的情况也能被正确区分。

#### 位置精确性
结合文件路径和行号信息，确保跨文件的代码段不会冲突。

#### 大小感知性
包含内容长度信息，防止内容截断导致的误判。

---

### 边界情况处理

#### 重复文件处理
当同一文件被多次扫描时，系统会：

1. 检查文件哈希是否发生变化。
2. 如果文件有修改，删除旧的索引点。
3. 重新生成新的代码段和哈希值。

#### 循环引用处理
对于递归结构或循环依赖，哈希机制确保：

- 每个唯一的代码段只被索引一次。
- 不会因为循环引用导致无限重复。

#### 并发安全
哈希集合 `seenSegmentHashes` 是文件级别的，每个文件解析过程独立：

- 不同文件之间不会相互影响。
- 同一文件内的重复段会被有效过滤。
